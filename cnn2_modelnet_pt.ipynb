{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa495b1",
   "metadata": {},
   "source": [
    "# Reproducing CNN2\n",
    "\n",
    "**Progetto Machine Learning**, A. A. 2024/2025\n",
    "\n",
    "\n",
    "Written by Marco Giunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087914e-2b05-45b2-bd23-22e2718faa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from modelnet_dataset import ModelNetDataset\n",
    "from torchinfo import summary\n",
    "\n",
    "# Define some hyper-parameters\n",
    "LEARNING_RATE = 1e-4\n",
    "DECAY_RATE = 0.96\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 320\n",
    "DROPOUT = 0.5\n",
    "SCALE_LIST = [1,3,5]\n",
    "\n",
    "# Set the random seed\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Send logs to Weights & Biases\n",
    "LOG2WANDB = False\n",
    "\n",
    "# Define the device where we want to run our mode\n",
    "# If a CUDA-capable machine is available, then set the GPU as running device\n",
    "# If a Apple ARM Mx-capable machine is available, then set the MPS as running device\n",
    "torch_device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch_device = 'mps'\n",
    "DEVICE = torch.device(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e642b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (LOG2WANDB):\n",
    "    import wandb\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"reproducing-cnn2-project\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"architecture\": \"CNN2\",\n",
    "        \"dataset\": \"ModelNet2D\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\" : BATCH_SIZE,\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"num_classes\": 5,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b363a-09f2-480a-beb6-0d69ef5d4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images inside the ModelNet2D dataset consists of 48.600 images of\n",
    "# size 144 × 144 pixels in 5 classes: chairs, human figures, cars, airplanes and lamp.\n",
    "# We have to first convert them into torch.Tensor and then downsample to 48 × 48 pixels\n",
    "transform = v2.Compose([\n",
    "                v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "                v2.Resize(size=(48,48))\n",
    "            ])\n",
    "\n",
    "# load the ModelNet2D dataset\n",
    "modelnet_dataset = ModelNetDataset(transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac18bdc",
   "metadata": {},
   "source": [
    "On the ModelNet2D dataset, the authors use the images taken from azimuths of degrees from 50 to 125 as the training set, degrees from 30 to 45 and from 130 to 145 as the validation set, and the rest as the test set. (4.1 3D Viewpoint Generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93477f7f-675b-416f-8a34-fbd2cc9dfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the ModelNet2D dataset there are 72 azimuths (0 to 360 every 5 degrees).\n",
    "train_azimuth_values = list(range(50,130,10))\n",
    "#train_azimuth_values = list(range(50,130,10)) + list(range(150,230,10))  # added more samples to train dataset\n",
    "validation_azimuth_values = [30,40,130,140]\n",
    "test_azimuth_values = [0, 10, 20] + list(range(150,360,10))\n",
    "#test_azimuth_values = [0, 10, 20] + list(range(230,360,10)) # removed samples from test dataset\n",
    "\n",
    "train_idx, validation_idx, test_idx = [], [], []\n",
    "for i, (images, label, azimuth) in enumerate(modelnet_dataset):\n",
    "\n",
    "    if azimuth in train_azimuth_values:\n",
    "        train_idx.append(i)\n",
    "    elif azimuth in validation_azimuth_values:\n",
    "        validation_idx.append(i)\n",
    "    else:\n",
    "        test_idx.append(i)\n",
    "\n",
    "train_set = torch.utils.data.Subset(modelnet_dataset, train_idx)\n",
    "validation_set = torch.utils.data.Subset(modelnet_dataset, validation_idx)\n",
    "test_set = torch.utils.data.Subset(modelnet_dataset, test_idx)\n",
    "\n",
    "# Classes which images belong to\n",
    "classes = ('airplane', 'car', 'chair', 'lamp', 'person')\n",
    "\n",
    "print('Number of training images: {}'.format(len(train_set)))\n",
    "print('Number of validation images: {}'.format(len(validation_set)))\n",
    "print('Number of test images: {}'.format(len(test_set)))\n",
    "\n",
    "# PyTorch data loaders\n",
    "# Loading data from disk and organize it in batches\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27606ba3",
   "metadata": {},
   "source": [
    "Show some sample, with label and azimuth value."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d0b1085-8bf4-4675-a097-67f49fc10aa9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Attempt to retrieve 5 items\n",
    "for i, (images, label, azimuth) in enumerate(train_dataloader):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.set_title('{} #{}L'.format(classes[label[i]], azimuth[i]))\n",
    "    ax1.imshow(torchvision.transforms.ToPILImage()(images[0][i].squeeze()), cmap='gray', vmin=0, vmax=255)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.set_title('{} #{}R'.format(classes[label[i]], azimuth[i]))\n",
    "    ax2.imshow(torchvision.transforms.ToPILImage()(images[1][i].squeeze()), cmap='gray', vmin=0, vmax=255)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7672fea",
   "metadata": {},
   "source": [
    "## CNN2 model\n",
    "Our CNN2 consists of 3 blocks. Each block has a CM pooling layer and a convolution layer. We use 3 scales (s= 0,1,2) in a CM pooling layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cbe98-b0f0-42cc-aa59-2e8fe0cd8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonocularLayer(nn.Module):\n",
    "    def __init__(self, filters, ksize, in_channels, pool_stride):\n",
    "        super(MonocularLayer, self).__init__()\n",
    "        self.out_channels = filters\n",
    "        self.kernel_size = ksize\n",
    "        self.in_channels = in_channels\n",
    "        self.pool_stride = pool_stride\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, stride=1, padding='same', device=DEVICE),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "           \n",
    "    # images = feature_maps\n",
    "    def forward(self, images):\n",
    "        pool_maps = self.cmpooling(images)\n",
    "        pool_maps = torch.concat(pool_maps, axis=1)\n",
    "        return self.conv(pool_maps.to(DEVICE))\n",
    "\n",
    "    ## Utility function\n",
    "    def cmpooling(self, images):\n",
    "        scale_list = SCALE_LIST\n",
    "        pool_stride = self.pool_stride\n",
    "        # make sure the scale_list is in decending order\n",
    "        # es. [1,3,5] => [5, 3, 1]\n",
    "        if scale_list[0] - scale_list[1] < 0:\n",
    "            scale_list = scale_list[::-1]\n",
    "        \n",
    "        # concentric multi-scale pooling\n",
    "        # es. scale_list=[5, 3, 1] => offset=[0, 1, 2]\n",
    "        offset = [0] + [-(scale_list[i+1] - scale_list[0])//2 for i in range(len(scale_list) - 1)]\n",
    "        pool_maps = []\n",
    "        for offset, scale in zip(offset, scale_list):\n",
    "            slice_maps = images[0:, # all batch \n",
    "                                0:, # all layer\n",
    "                                offset:offset + images.shape[2] - offset * 2, # image witdh\n",
    "                                offset:offset + images.shape[3] - offset * 2] # image height\n",
    "            # es.\n",
    "            # images = [32, 2, 48, 48]\n",
    "            # offset = 0\n",
    "            # slice_maps = [32, 2, 0:48, 0:48]\n",
    "            # \n",
    "            # offset = 1\n",
    "            # slice_maps = [32, 2, 1:47, 1:47]\n",
    "            #\n",
    "            # offset = 2\n",
    "            # slice_maps = [32, 2, 2:46, 2:46]\n",
    "            pool_map = F.max_pool2d(input=slice_maps, kernel_size=scale, stride=pool_stride)\n",
    "            pool_maps.append(pool_map)\n",
    "    \n",
    "        # assert same shape for all pool_map\n",
    "        for i in range(len(pool_maps)-1):\n",
    "            assert pool_maps[i].shape[1:] == pool_maps[-1].shape[1:]\n",
    "        return pool_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007fa80f",
   "metadata": {},
   "source": [
    "The first, second, and third convolution layers have 6, 12, and 32 filters with sizes 5 × 5, 5 × 5, and 3 × 3 respectively. All filters use the stride of 1, and each unit uses the ReLU non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5127180-6b0f-4be8-932a-aff8a03a844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=64, kernel_size=1, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=DROPOUT),\n",
    "            nn.Linear(9216, num_classes) # cross entropy loss needs logits, do not use SoftMax\n",
    "        )\n",
    "\n",
    "        self.monocular1l = MonocularLayer(filters=6, ksize=5, in_channels=6, pool_stride=2)\n",
    "        self.monocular1r = MonocularLayer(filters=6, ksize=5, in_channels=6, pool_stride=2)\n",
    "        self.monocular2l = MonocularLayer(filters=12, ksize=5, in_channels=36, pool_stride=1)\n",
    "        self.monocular2r = MonocularLayer(filters=12, ksize=5, in_channels=36, pool_stride=1)\n",
    "        self.monocular3l = MonocularLayer(filters=32, ksize=3, in_channels=72, pool_stride=1)\n",
    "        self.monocular3r = MonocularLayer(filters=32, ksize=3, in_channels=72, pool_stride=1)\n",
    "\n",
    "    def forward(self, left_eye, right_eye):\n",
    "    \n",
    "        # parallax augmentation\n",
    "        parallax = left_eye - right_eye\n",
    "\n",
    "        left = torch.concat([left_eye, -parallax], axis=1) # Size(2, 48, 48)\n",
    "        right = torch.concat([right_eye, parallax], axis=1) # Size(2, 48, 48)\n",
    "\n",
    "        left1 = self.monocular1l(left) # Size(6, 22, 22)\n",
    "        right1 = self.monocular1r(right) # Size(6, 22, 22)\n",
    "        \n",
    "        left2 = self.monocular2l(torch.concat([left1, right1], axis=1)) # Size(12, 18, 18)\n",
    "        right2 = self.monocular2r(torch.concat([right1, left1], axis=1)) # Size(12, 18, 18)\n",
    "\n",
    "        left3 = self.monocular3l(torch.concat([left2, right2], axis=1)) # Size(32, 14, 14)\n",
    "        right3 = self.monocular3r(torch.concat([right2, left2], axis=1)) # Size(32, 14, 14)\n",
    "        \n",
    "        merge_binocular = torch.concat([left3, right3], axis=1)\n",
    "\n",
    "        merge_binocular = self.conv(merge_binocular)\n",
    "        output = self.fc(merge_binocular)\n",
    "    \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec55560f-278d-49cf-bdff-6afb582df49d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Attempt to retrieve CNN2 items\n",
    "torch.set_grad_enabled(False)\n",
    "model = CNN2(num_classes=5)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "monocular1 = MonocularLayer(filters=6, ksize=5, in_channels=6, pool_stride=2)\n",
    "monocular2 = MonocularLayer(filters=12, ksize=5, in_channels=36, pool_stride=1)\n",
    "monocular3 = MonocularLayer(filters=32, ksize=3, in_channels=72, pool_stride=1)\n",
    "\n",
    "for i, (images, label, azimuth) in enumerate(train_dataloader):\n",
    "    if i == 2:\n",
    "        break\n",
    "    left_eye = images[0]\n",
    "    right_eye = images[1]\n",
    "    # parallax augmentation\n",
    "    parallax = left_eye - right_eye\n",
    "\n",
    "    left = torch.concat([left_eye, -parallax], axis=1) # Size(2, 48, 48)\n",
    "    right = torch.concat([right_eye, parallax], axis=1) # Size(2, 48, 48)\n",
    "\n",
    "    left1 = monocular1(left) # Size(6, 22, 22)\n",
    "    right1 = monocular1(right) # Size(6, 22, 22)\n",
    "        \n",
    "    left2 = monocular2(torch.concat([left1, right1], axis=1)) # Size(12, 18, 18)\n",
    "    right2 = monocular2(torch.concat([right1, left1], axis=1)) # Size(12, 18, 18)\n",
    "\n",
    "    left3 = monocular3(torch.concat([left2, right2], axis=1)) # Size(32, 14, 14)\n",
    "    right3 = monocular3(torch.concat([right2, left2], axis=1)) # Size(32, 14, 14)\n",
    "    x = torch.concat([left3, right3], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,4,1)\n",
    "    ax1.set_title('L')\n",
    "    ax1.imshow(torchvision.transforms.ToPILImage()(left_eye[0][0].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    ax2 = fig.add_subplot(1,4,2)\n",
    "    ax2.set_title('L-R')\n",
    "    ax2.imshow(torchvision.transforms.ToPILImage()(parallax[0][0].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    ax3 = fig.add_subplot(1,4,3)\n",
    "    ax3.set_title('R')\n",
    "    ax3.imshow(torchvision.transforms.ToPILImage()(right_eye[0][0].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    ax4 = fig.add_subplot(1,4,4)\n",
    "    ax4.set_title('R-L')\n",
    "    ax4.imshow(torchvision.transforms.ToPILImage()(left[0][1].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    fig2 = plt.figure()\n",
    "    bx1 = fig2.add_subplot(1,6,1)\n",
    "    bx1.set_title('Out 1')\n",
    "    bx1.imshow(torchvision.transforms.ToPILImage()(left1[0][0].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    bx2 = fig2.add_subplot(1,6,2)\n",
    "    bx2.set_title('Out 2')\n",
    "    bx2.imshow(torchvision.transforms.ToPILImage()(left1[0][1].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    bx3 = fig2.add_subplot(1,6,3)\n",
    "    bx3.set_title('Out 3')\n",
    "    bx3.imshow(torchvision.transforms.ToPILImage()(left1[0][2].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    bx4 = fig2.add_subplot(1,6,4)\n",
    "    bx4.set_title('Out 4')\n",
    "    bx4.imshow(torchvision.transforms.ToPILImage()(left1[0][3].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    bx5 = fig2.add_subplot(1,6,5)\n",
    "    bx5.set_title('Out 5')\n",
    "    bx5.imshow(torchvision.transforms.ToPILImage()(left1[0][4].squeeze()),cmap='gray', vmin=0, vmax=255)\n",
    "    bx6 = fig2.add_subplot(1,6,6)\n",
    "    bx6.set_title('Out 6')\n",
    "    bx6.imshow(torchvision.transforms.ToPILImage()(left1[0][5].squeeze()),cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484047a8",
   "metadata": {},
   "source": [
    "Show model summary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a072949-edc2-4a3f-9aa9-fab48823f275",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model = CNN2(num_classes=5)\n",
    "model = model.to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "for i, (images, label, azimuth) in enumerate(train_dataloader):\n",
    "    if i == 1:\n",
    "        break\n",
    "    left_eye = images[0]\n",
    "    right_eye = images[1]\n",
    "    s = summary(model, input_data=[left_eye, right_eye])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31513a-8e3b-4e1f-aeb6-3b88b6e77cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2(num_classes=5)\n",
    "\n",
    "# Move the model to the right device\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=DECAY_RATE, last_epoch=-1)\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "  \"\"\"\n",
    "  Computes the accuracy between preds and labels\n",
    "\n",
    "  preds:\n",
    "    torch.tensor of size (B, N) where B is the batch size\n",
    "    and N is the number of classes\n",
    "    it contains the predicted probabilities for each class\n",
    "  labels:\n",
    "    torch.tensor of size (B) where each item is an integer\n",
    "    taking value in [0,N-1]\n",
    "\n",
    "  Returns:\n",
    "    the accuracy between preds and labels\n",
    "  \"\"\"\n",
    "  _, pred_idxs = torch.max(preds.data, 1)\n",
    "  correct = (pred_idxs == labels).sum()\n",
    "  total = labels.size(0)\n",
    "  return float(correct) / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9629a",
   "metadata": {},
   "source": [
    " We use early stopping strategy for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.2):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the quantity monitored not improving\n",
    "        self.min_monitor = float(\"Inf\") # Quantity to be monitored\n",
    "\n",
    "    # return True when quantity monitored is not decreased by the `min_delta` for `patience` times \n",
    "    def early_stop_check(self, monitor):\n",
    "        if ((monitor + self.min_delta) < self.min_monitor):\n",
    "            self.min_monitor = monitor\n",
    "            self.counter = 0  # reset the counter if quantity monitored decreased at least by min_delta\n",
    "        elif ((monitor + self.min_delta) > self.min_monitor):\n",
    "            self.counter += 1 # increase the counter if quantity monitored is not decreased by the min_delta\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        #print('Counter: {}'.format(self.counter))   \n",
    "        return False\n",
    "    \n",
    "early_stopping = EarlyStopping(patience=20, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f25916-7610-4e90-ba96-99a3b869dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "if (LOG2WANDB):\n",
    "    wandb.watch(model, log_freq=100)\n",
    "for e in range(EPOCHS):\n",
    "    train_acc=0\n",
    "    train_loss=0\n",
    "    for i, (images, label, azimuth) in enumerate(train_dataloader):\n",
    "        # Let's set our model in training modality\n",
    "        model.train()\n",
    "        left_eyes = images[0]\n",
    "        right_eyes = images[1]\n",
    "        labels = label.type(torch.LongTensor)\n",
    "        \n",
    "        # Move the data to the right device\n",
    "        left_eyes = left_eyes.to(DEVICE)\n",
    "        right_eyes = right_eyes.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Perform the forward pass with CNN2\n",
    "        out = model(left_eyes, right_eyes).to(DEVICE)\n",
    "\n",
    "        # Define the loss function\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "\n",
    "        # Use torch.softmax to obtain probabilities over the logits (nn.CrossEntropyLoss expects raw logits as the model output)\n",
    "        acc = accuracy(torch.softmax(out, dim=1), labels)\n",
    "\n",
    "        train_acc += acc\n",
    "        train_loss += loss.item()\n",
    "        # Perform the update of the model's parameter using the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Let's print some info about the training\n",
    "        if i % 100 == 0:\n",
    "            print('Loss: {:.05f} - Accuracy {:.05f}'.format(loss.item(), acc))\n",
    "    lr_scheduler.step()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    if (LOG2WANDB):\n",
    "        # Where the magic happens\n",
    "        wandb.log({\"train-loss\": train_loss, \"train-accuracy\": train_acc}, commit=False)\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_acc = 0\n",
    "        val_loss = 0\n",
    "        for epoch, (images, label, azimuth) in enumerate(validation_dataloader):\n",
    "            validation_left_eyes = images[0]\n",
    "            validation_right_eyes = images[1]\n",
    "            validation_labels = label.type(torch.LongTensor)\n",
    "        \n",
    "            # Move the data to the right device\n",
    "            validation_left_eyes = validation_left_eyes.to(DEVICE)\n",
    "            validation_right_eyes = validation_right_eyes.to(DEVICE)\n",
    "            validation_labels = validation_labels.to(DEVICE)\n",
    "\n",
    "            # Perform the forward pass with CNN2\n",
    "            validation_out = model(validation_left_eyes, validation_right_eyes).to(DEVICE)\n",
    "            # Define the loss function\n",
    "            validation_loss = F.cross_entropy(validation_out, validation_labels)\n",
    "            # Use torch.softmax to obtain probabilities over the logits\n",
    "            validation_accuracy = accuracy(torch.softmax(validation_out, dim=1), validation_labels)\n",
    "            \n",
    "            val_acc += validation_accuracy\n",
    "            val_loss += validation_loss.item()\n",
    "            # remember best accuracy and save checkpoint\n",
    "            #is_best = validation_accuracy > best_accuracy\n",
    "            #best_accuracy = max(validation_accuracy, best_accuracy)\n",
    "            is_best = True\n",
    "            best_accuracy = validation_accuracy\n",
    "            model_state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss' : validation_loss,\n",
    "            }\n",
    "            torch.save(model_state, 'cnn2_checkpoint.pth.tar')\n",
    "            if is_best:\n",
    "                shutil.copyfile('cnn2_checkpoint.pth.tar', 'cnn2_model_best.pth.tar')\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Validation Loss: {:.05f} - Accuracy {:.05f}'.format(validation_loss.item(), validation_accuracy))\n",
    "        val_loss /= len(validation_dataloader)\n",
    "        val_acc /= len(validation_dataloader)\n",
    "    if (LOG2WANDB):\n",
    "        # Where the magic happens\n",
    "        wandb.log({\"val-loss\": val_loss, \"val-accuracy\": val_acc})\n",
    "    # early stopping\n",
    "    if early_stopping.early_stop_check(val_loss):\n",
    "       break            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e2c5e-3f8d-4181-897f-5bce552eb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "best_model = torch.load('cnn2_model_best.pth.tar', weights_only=True)\n",
    "model.load_state_dict(best_model['model_state_dict'])\n",
    "# Let's set our model in evaluation modality\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_acc = 0\n",
    "    test_l = 0\n",
    "    for step, (images, label, azimuth) in enumerate(test_dataloader):\n",
    "        left_eyes = images[0]\n",
    "        right_eyes = images[1]\n",
    "        labels = label.type(torch.LongTensor)\n",
    "        \n",
    "        # Move the data to the right device\n",
    "        left_eyes = left_eyes.to(DEVICE)\n",
    "        right_eyes = right_eyes.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Perform the forward pass with CNN2\n",
    "        test_out = model(left_eyes, right_eyes).to(DEVICE)\n",
    "\n",
    "        # Define the loss function\n",
    "        test_loss = F.cross_entropy(test_out, labels)\n",
    "\n",
    "         # Use torch.softmax to obtain probabilities over the logits\n",
    "        test_accuracy = accuracy(torch.softmax(test_out, dim=1), labels)\n",
    "\n",
    "        test_acc += test_accuracy\n",
    "        test_l += test_loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print('Test Loss: {:.05f} - Accuracy {:.05f}'.format(test_loss.item(), test_accuracy))\n",
    "if (LOG2WANDB):            \n",
    "    # Where the magic happens\n",
    "    wandb.log({\"test-loss\": test_l / len(test_dataloader), \"test-accuracy\": test_acc / len(test_dataloader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (LOG2WANDB):\n",
    "    # Mark the run as finished\n",
    "    wandb.finish()\n",
    "print('Training done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
